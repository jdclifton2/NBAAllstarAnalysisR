---
title: "Predicting The NBA MVP"
author: "Mohammed Bukhattala, Justin Clifton, Maxwell Pitney, Graham Swain"
date: "05/04/2021"
abstract: "The NBA most valuable player award is given to one player every NBA season. There have been 65 different players who have won this award. In this paper, we offer several models that attempt to predict the NBA MVP. These models will be using individual player data tracked over the course of a season. "
output: html_document
---

# Introduction

The most prized individual accolade for a player in the NBA is the Most Valuable Player award. Winning this award signifies that coaches, the media, and former players thought that the winner was the most important player in the NBA. There have been 65 different players who have won this award. These players played different positions, had different statistics, and come from different eras. What then, do they have in common? Given player statistics, can we accurately predict what player will win the MVP award in a given year? In this paper, we attempt to answer that very question. In order to do this, we frame the problem in terms of classification. Thus, we will create binary classifiers that, when given player statistics can classify that player as an MVP or not. Some models we will be using include LDA, KNN, and decision trees.

# Data Source and Variable Description

We will be considering data about player statistics tracked over the course of a season. Our data is a combination of two other datasets. The first is a dataset of NBA season statistics from the years 1950-2017 contained at https://www.kaggle.com/drgilermo/nba-players-stats. The data in this dataset was scraped from www.basketball-reference.com, a website dedicated to tracking basketball statistics. In order to get the players that won the MVP award, more data was scraped from basketball-reference using the beautifulSoup python package. These two datasets were then combined to form the one that is used in this project. 

### Variable Description

Since the data was pulled from basketball-reference, descriptions of variables can be found on their website at:
https://www.basketball-reference.com/about/glossary.html



```{r setup, include=FALSE}
library(readr)
library(tibble)
library(ggplot2)
#library(GGally)
library(dplyr)
library(corrplot)
library(caret)
library(class)
library(rpart)
library(rpart.plot)
library(randomForest)
library(rsample)



set.seed(23)
df2020 = as_tibble(read.csv("2020stats.csv", header = TRUE))
advanced2020 = as_tibble(read.csv("advanced.csv", header = TRUE))
nbaDf2020 = dplyr::inner_join(df2020, advanced2020)
names(nbaDf2020)
DF = read.csv("NBA_Stats_MVP.csv", header = TRUE)
nba_df = na.omit(DF)
names(nba_df)
nba_df$was_mvp = as.logical(nba_df$was_mvp)
nbaTib = as_tibble(nba_df)
cleanDF = dplyr::select(nbaTib, -c(Pos, Tm, Name, Year, is_allstar))
attach(cleanDF)
names(nbaTib)
```

# Methods

### Scoring Our Models

Before we can discuss the actual modeling of the problem, it is important to understand how the accuracy of models will be assessed. In a classification problem, there a few metrics that can be considered. Some of the metrics that will be considered are standard to all classification problems such as accuracy, true positive rate, false positive rate, true negative rate, and false negative rate. In the context of our problem domain, a true positive happens when a player was predicted as the MVP, and actually won the MVP. A true negative happens when a player was predicted as a non-MVP and was not the MVP. A false positive occurs when a player was predicted as the MVP but did not win the MVP. A false negative occurs when a player was predicted as a non-MVP but actually won the MVP. The metric of most interest for this problem, is the true positive rate. It is important to note that many of the models contained in this paper will have very high accuracy. This is due to the fact that there are so few players who win the MVP and so many that do not win that any model can simply guess non-MVP for every player and achieve high accuracy. On this account, we will be using metrics other than accuracy to score our models. These metrics are the f1 score and Matthews Correlation Coefficient (MCC). The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal (scikit).

The MCC produces a high score only if the prediction obtained good results in all of the four confusion matrix categories (true positives, false negatives, true negatives, and false positives), proportionally both to the size of positive elements and the size of negative elements in the dataset (Chicco, Gurman). 

These metrics will be calculated using the following functions where table is the confusion matrix:

```{r scoringFuncs, include = TRUE}
truePositive <- function(table) {
  tp = table[2,2]
  tn = table[1,1]
  fp = table[1,2]
  fn = table[2,1]
  return(tp/(tp + fn))
}

falseNegative <- function(table) {
  tp = table[2,2]
  tn = table[1,1]
  fp = table[1,2]
  fn = table[2,1]
  return(fn/(tp + fn))
}

falsePositive <- function(table) {
  tp = table[2,2]
  tn = table[1,1]
  fp = table[1,2]
  fn = table[2,1]
  return(fp/(fp + tn))
}

f1Score <- function(table) {
  tp = table[2,2]
  tn = table[1,1]
  fp = table[1,2]
  fn = table[2,1]
  return(2*tp/(2 * tp + fp + fn))
}

mcc <- function(table) {
  tp = as.double(table[2,2])
  tn = as.double(table[1,1])
  fp = as.double(table[1,2])
  fn = as.double(table[2,1])
  divisor = sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))
  return(((tp * tn) - (fp * fn))/divisor)
}


```

# Validating the Data

The first problem to consider is how to split the data into training and testing sets. Initially, we simply split the data randomly. However, while this method may be valid in most contexts, it is not the most appropriate method for our problem domain. As discussed previously, a very small amount of players have won the MVP award, and our data contains an even smaller subset of those players. Therefore, if random sampling is performed it is entirely possible that our training sets contain 0 MVP players. For this reason, we have instead decided in some cases to partition the data based on the year with training set given by years prior to 2000 and testing set given by years later than 2000. 


# Plots and Data Analysis

First, we will do some preliminary data analysis and consider a few plots of the data. In our first plot, we produce a scatter plot displaying field goal attempts (i.e. the number of shots taken) on our x axis, and the number of points scored on the y axis. Red circles indicate players who did not win the MVP award, and blue triangles correspond with players who did win. As can be seen from the plot, MVP players tend to score higher and take more shots. 

```{r fgavspts, echo=FALSE}
ggplot(data = nbaTib) +
  geom_point(mapping = aes(x = FGA, y = PTS, color = was_mvp, shape=was_mvp))
```

Next, we consider a breakdown of all MVP players by their position. This data comes from seasons between 1974 - 2017. We see that the center position has the most MVP players between this time frame. 

```{r mvphist1, echo=FALSE}
ggplot(dplyr::filter(DF, was_mvp == 1), aes(x = factor(Pos), fill = Pos)) +
    geom_bar() +
    scale_y_continuous(breaks=c(4,6,8,10,12)) +
    labs(title="NBA MVPS by Position (1974 - 2017)",
        x ="Position", y = "frequency")
```

Unfortunately, statistics were not tracked properly prior to 1983, and therefor we omit this data from consideration. Below we display a histogram corresponding to all MVP players where accurate data was collected. 

```{r mvphist2, echo=FALSE}
ggplot(dplyr::filter(nbaTib, was_mvp == 1), aes(x = Pos, fill = Pos)) +
    geom_bar() +
    scale_y_continuous(breaks=c(4,6,8,10)) +
    labs(title="NBA MVPS by Position (1983 - 2017)",
        x ="Position", y = "frequency")
```


# Modeling and Results


```{r datasplitting, include=FALSE}
train = sample(dim(cleanDF)[1], dim(cleanDF)[1]/2)
test = cleanDF[-train, ]
dfLT2000 = dplyr::filter(nbaTib, Year <= 2000)
dfLT2000 = dplyr::select(dfLT2000, -c(Pos, Tm, Name, Year, is_allstar))
dfGT2000 = dplyr::filter(nbaTib, Year > 2000)
dfGT2000 = dplyr::select(dfGT2000, -c(Pos, Tm, Name, Year, is_allstar) )
```

### LDA

The first model that we will consider is an LDA model. It is important to remember that one of the assumptions of LDA is that none of the features are heavily correlated. We will first examine a correlation matrix of our data. 
```{r originalCorr, echo=FALSE}
corMat = cor(cleanDF)
corrplot(corMat, tl.cex=0.5,method="shade")
```

It can be seen that there are many heavily correlated features. We will now drop features with correlation higher or lower than than |.7|. 
```{r dropCorr, include=FALSE}
names = findCorrelation(corMat, cutoff = .7, verbose = TRUE, names = TRUE)

nonCorDF = dplyr::select(cleanDF, -all_of(names))
```

We now offer the new correlation matrix for potential features of the LDA model. 
```{r newCorr, echo= FALSE}
corMat2 = cor(nonCorDF)

corrplot(corMat2, tl.cex=0.5,method="shade")
```


```{r corMat3, include = FALSE}
corMat3 = cor(dfLT2000)
names = findCorrelation(corMat3, cutoff = .7, verbose = TRUE, names = TRUE)
```

With heavily correlated features removed, an LDA model can be considered. This model was fit on a random split of the data. All non correlated variables were used to predict if the MVP.
```{r lda, include=FALSE}
nonCorDfLT2000 = dplyr::select(dfLT2000, -all_of(names))
nonCorDfGT2000 = dplyr::select(dfGT2000, -all_of(names))

library(MASS)
trainNonCor = sample(dim(nonCorDF)[1], dim(nonCorDF)[1]/2)
testNonCor = nonCorDF[-trainNonCor, ]
ldaFit = lda(was_mvp~., data = nonCorDF, subset=train)
ldaPred = predict(ldaFit, testNonCor)
ldaTable = table(ldaPred$class,testNonCor$was_mvp)
accuracy = mean(ldaPred$class == testNonCor$was_mvp)
```

#### First LDA Results
```{r}
ldaTable
accuracy
truePositive(ldaTable)
falseNegative(ldaTable)
falsePositive(ldaTable)
f1Score(ldaTable)
mcc(ldaTable)
```
The first model had many high false positives. The low true positive rate and MCC reflects that.

```{r lda2, include=FALSE}
ldaFit = lda(was_mvp ~ ., data = nonCorDfLT2000)
ldaPred = predict(ldaFit, nonCorDfGT2000)
ldaTable = table(ldaPred$class,dfGT2000$was_mvp)
accuracy = mean(ldaPred$class == dfGT2000$was_mvp)
```

#### Second LDA Results

This model was fit on years prior to 2000 and tested on data from years greater than 2000.
```{r}
ldaTable
accuracy
truePositive(ldaTable)
falseNegative(ldaTable)
falsePositive(ldaTable)
f1Score(ldaTable)
mcc(ldaTable)
```
The second model performed marginally worse. The new training and testing set did improve the false negatives, however, the true positive and MCC were slightly lowered.

### LOOCV LDA
We will now perform an LDA using leave one out cross validation on the original training and testing set. 
```{r ldaloo, include=FALSE}
ldaFit = lda(was_mvp~., data = nonCorDF, CV = TRUE)
ldaTable = table(ldaFit$class, nonCorDF$was_mvp)
accuracy = mean(ldaFit$class ==  nonCorDF$was_mvp)
```

```{r results1, message=FALSE}
ldaTable
accuracy
truePositive(ldaTable)
falseNegative(ldaTable)
falsePositive(ldaTable)
f1Score(ldaTable)
mcc(ldaTable) 
```
Leave one out cross validation further worsened performance.


### KNN
We will now perform KNN using the randomly split training and testing sets.
```{r knn, include=FALSE}
names(cleanDF)
trainX = cbind(Age, G, GS, MP, PER, TS., X3PAr, FTr, ORB., DRB., TRB.,
               AST., STL., BLK., TOV., USG., OWS, DWS, WS, WS.48, OBPM, DBPM,
               BPM, VORP, FG, FGA, FG., X3P, X3PA, X3P., X2P, X2PA, X2P.,
               eFG., FT, FTA, FT., ORB, DRB, TRB, AST, STL, BLK, TOV, PF, PTS
               )[train, ]
testX = cbind(Age, G, GS, MP, PER, TS., X3PAr, FTr, ORB., DRB., TRB.,
               AST., STL., BLK., TOV., USG., OWS, DWS, WS, WS.48, OBPM, DBPM,
               BPM, VORP, FG, FGA, FG., X3P, X3PA, X3P., X2P, X2PA, X2P.,
               eFG., FT, FTA, FT., ORB, DRB, TRB, AST, STL, BLK, TOV, PF, PTS)[-train, ]
trainY = cleanDF[train, ]$was_mvp
testY = cleanDF[-train, ]$was_mvp
```

```{r knn1, include=FALSE}
knnPred1 = knn(trainX, testX, trainY, k=1)
accuracy = mean(knnPred1 == testY)
knnTable1 = table(knnPred1, testY)
```

```{r}
knnTable1
accuracy
truePositive(knnTable1)
falseNegative(knnTable1)
falsePositive(knnTable1)
f1Score(knnTable1)
mcc(knnTable1)
```

```{r knn2, include=FALSE}
knnPred2 = knn(trainX, testX, trainY, k=2)
accuracy = mean(knnPred2 == testY)
knnTable2 = table(knnPred2, testY)
```

```{r}
knnTable2
accuracy
truePositive(knnTable2)
falseNegative(knnTable2)
falsePositive(knnTable2)
f1Score(knnTable2)
mcc(knnTable2)
```


```{r knn3, include=FALSE}
knnPred3 = knn(trainX, testX, trainY, k=3)
accuracy = mean(knnPred3 == testY)
knnTable3 = table(knnPred3, testY)
```

```{r}
knnTable3
accuracy
truePositive(knnTable3)
falseNegative(knnTable3)
falsePositive(knnTable3)
f1Score(knnTable3)
mcc(knnTable3)
```


```{r knn4, include=FALSE}
knnPred4 = knn(trainX, testX, trainY, k=4)
accuracy = mean(knnPred4 == testY)
knnTable4 = table(knnPred4, testY)
```

```{r}
knnTable4
accuracy
truePositive(knnTable4)
falseNegative(knnTable4)
falsePositive(knnTable4)
f1Score(knnTable4)
mcc(knnTable4)
```


```{r knn6, include=FALSE}
# trainX = dfLT2000[, -47]
# testX = dfGT2000[,-47]
# trainY = dfLT2000$was_mvp
# testY = dfGT2000$was_mvp
# 
# 
# knnPred = knn(trainX, testX, trainY, k=1)
# accuracy = mean(knnPred == testY)
# knnTable1 = table(knnPred, testY)
# 
# knnTable1
# accuracy
# truePositive(knnTable1)
# falseNegative(knnTable1)
# f1Score(knnTable1)
# mcc(knnTable1)
# 
# 
# knnPred2 = knn(trainX,testX,trainY, k=3)
# accuracy = mean(knnPred2 == testY)
# knnTable2 = table(knnPred2, testY)
# 
# knnTable2
# accuracy
# truePositive(knnTable2)
# falseNegative(knnTable2)
# f1Score(knnTable2)
# mcc(knnTable2)
# 
# 
# knnPred3 = knn(trainX, testX, trainY, k=4)
# mean(knnPred3 == testY)
# knnTable3 = table(knnPred3, testY)
# 
# knnTable3
# accuracy
# truePositive(knnTable3)
# falseNegative(knnTable3)
# f1Score(knnTable3)
# mcc(knnTable3)
# 
# 
# knnPred4 = knn(trainX,testX,trainY,k=2)
# mean(knnPred4 == testY)
# knnTable4 = table(knnPred4, testY)
# 
# knnTable4
# accuracy
# truePositive(knnTable4)
# falseNegative(knnTable4)
# f1Score(knnTable4)
# mcc(knnTable4)
```

We find that for knn, a k = 2 is the most effective model with an mcc score of
$32\%$. That being said, none of these models are particularly effective.

### Random Forest

Next, a random forest is fit to the data. This random forest serves two purposes. First, it will act as a sufficient model but it will also be used to select important features. 

```{r forest, include=FALSE}
treeSplit = initial_split(cleanDF, prop = .5)
treeTrain = training(treeSplit)
treeTest  = testing(treeSplit)

treeTrain$was_mvp = as.character(treeTrain$was_mvp)
treeTrain$was_mvp = as.factor(treeTrain$was_mvp)

mvpForest = randomForest(was_mvp ~ ., data = treeTrain, importance = TRUE)
mvpForest

predTest = predict(mvpForest, treeTest, type = "class")
mvpForest$importance
mvpForest$confusion
accuracy = mean(predTest == treeTest$was_mvp)
forestTable = table(predTest, treeTest$was_mvp)
```


```{r}
varImpPlot(mvpForest, sort=TRUE)
forestTable
accuracy
truePositive(forestTable)
falseNegative(forestTable)
falsePositive(forestTable)
f1Score(forestTable)
mcc(forestTable)
```

In the plot above, we see some of the important features are the advanced statistics BPM, Win Shares and Value over replacement. We will use these and some of the other important features to fit more models. 

We now consider logistical regression models. 

### Logistical Regression

The first model will use all uncorrelated features.

```{r logistic, include=FALSE}
logFit <- glm(was_mvp ~ ., data = nonCorDF[trainNonCor, ], family = binomial)

#summary(logFit)

probLog = predict(logFit, newdata = testNonCor, type = "response")

predLog = ifelse(probLog > 0.5, TRUE, FALSE)
acuracy = mean(predLog == testNonCor$was_mvp)
logTable = table(predLog, testNonCor$was_mvp)

```

```{r}
logTable
accuracy
truePositive(logTable)
falseNegative(logTable)
falsePositive(logTable)
f1Score(logTable)
mcc(logTable)
```


Now, we will begin using features that were indicated as important by the random forest.

This model uses VORP + WS + BPM + PER + PTS + AST + TRB + BLK + STL + TOV + GS.

```{r logistic2, include=FALSE}
logFit2 <- glm(was_mvp ~ VORP + WS + BPM + PER + PTS + AST + TRB + BLK + STL + TOV + GS , data = cleanDF[train, ], family = binomial)

#summary(logFit)

probLog2 = predict(logFit2, newdata = test, type = "response")

predLog2 = ifelse(probLog2 > 0.5, TRUE, FALSE)
accuracy = mean(predLog2 == test$was_mvp)

logTable = table(predLog2, test$was_mvp)
```

```{r}
logTable
accuracy
truePositive(logTable)
falseNegative(logTable)
falsePositive(logTable)
f1Score(logTable)
mcc(logTable)
```


This model uses VORP + WS + BPM + PER + PTS + AST + TRB + BLK + STL + TOV + GS and is fit on the data split by year.

```{r logistic3, include=FALSE}
logFit2 <- glm(was_mvp ~ VORP + WS + BPM + PER + PTS + AST + TRB + BLK + STL + TOV + GS , data = dfLT2000, family = binomial)

#summary(logFit)

probLog2 = predict(logFit2, newdata = dfGT2000, type = "response")

predLog2 = ifelse(probLog2 > 0.5, TRUE, FALSE)
accuracy = mean(predLog2 == dfGT2000$was_mvp)

logTable = table(predLog2, dfGT2000$was_mvp)
```

```{r}
logTable
accuracy
truePositive(logTable)
falseNegative(logTable)
falsePositive(logTable)
f1Score(logTable)
mcc(logTable)
```

This model will be making use of interaction between some of the features and uses VORP + WS * OWS * WS.48 + BPM + PER + PTS * FG. + AST + TRB + BLK + STL + TOV + GS.

```{r logistic4, include=FALSE}
logFit2 <- glm(was_mvp ~ VORP + WS * OWS * WS.48 + BPM + PER + PTS * FG. + AST + TRB + BLK + STL + TOV + GS , data = dfLT2000, family = binomial)

#summary(logFit)

probLog2 = predict(logFit2, newdata = dfGT2000, type = "response")

predLog2 = ifelse(probLog2 > 0.8, TRUE, FALSE)
accuracy = mean(predLog2 == dfGT2000$was_mvp)

logTable = table(predLog2, dfGT2000$was_mvp)
```

```{r}
logTable
accuracy
truePositive(logTable)
falseNegative(logTable)
falsePositive(logTable)
f1Score(logTable)
mcc(logTable)
```


# Conclusion

This research compared different classification machine learning algorithms with the goal to predict the NBA MVP. This was done using data scraped from basketball-reference.com. Through the course of building and testing various classification models, we offer models that can accurately predict non-MVP players, and predict MVP winners moderately well. 

The nature of the MVP award means That only one player per season gets chosen. This introduces issues as there is an inherently limited number of MVP players. Being such a rare event led to all models having high accuracy, but low true to middle true positive rates. Various data splitting methods and multiple models were explored, however none were particularly successful. Logistical regression using the features found by random forests as well as including interaction terms offered the best model with an MCC of 0.60.

It is the speculation of the authors that superior models could be found if the data contained features involving team success (games won by the team) and popularity of the players (twitter followers). Another change that could yield more accurate results would be to, instead of classifying players as MVP or not, consider using regression to predict the number of MVP votes that each player received. These speculations serve as recommendations for any future research. 

# References

Chicco, D., Jurman, G. The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation. BMC Genomics 21, 6 (2020). https://doi.org/10.1186/s12864-019-6413-7

“Sklearn.metrics.f1_score.” Scikit, scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html. 